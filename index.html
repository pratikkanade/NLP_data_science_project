
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>H-1B Visa Approval Prediction Machine Learning Project</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="rag-data-pipeline"
                  title="H-1B Visa Approval Prediction Machine Learning Project"
                  environment="web"
                  feedback-link="https://your-feedback-link.com">
    
      <google-codelab-step label="Introduction" duration="0">
        <p>Welcome to the H-1B Visa Approval Prediction Machine Learning project codelab! In this tutorial, you will learn how to build a comprehensive machine learning pipeline to predict the approval status of H-1B visa applications. This project covers data loading, preprocessing, feature engineering, model training, evaluation, and deployment.</p>
<h2 class="checklist" is-upgraded>What you&#39;ll learn</h2>
<ul class="checklist">
<li>How to explore and preprocess a real-world dataset</li>
<li>Techniques to handle high cardinality categorical features</li>
<li>Methods to address class imbalance using SMOTE</li>
<li>Building and evaluating multiple classification models</li>
<li>Saving and deploying a trained model</li>
</ul>
<h2 is-upgraded>Prerequisites</h2>
<ul>
<li>Basic knowledge of Python and pandas</li>
<li>Familiarity with machine learning concepts</li>
<li>Jupyter Notebook environment (optional but recommended)</li>
</ul>
<p>Let&#39;s get started!</p>


      </google-codelab-step>
    
      <google-codelab-step label="Setup" duration="0">
        <h2 is-upgraded>Step 1: Clone the repository and prepare your environment</h2>
<pre><code language="language-bash" class="language-bash"># Clone the repository (replace URL with actual repo if available)
git clone https://github.com/yourusername/h1b-visa-approval-prediction.git
cd h1b-visa-approval-prediction

# Create and activate a virtual environment (optional but recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`

# Install required packages
pip install -r requirements.txt
</code></pre>
<p><strong>Note:</strong> The dataset file <code>h1b_2011_2016.csv</code> should be placed in the project root directory.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Data Loading and Initial Exploration" duration="0">
        <h2 is-upgraded>Step 2: Load the dataset</h2>
<p>Load the H-1B visa application data using pandas.</p>
<pre><code language="language-python" class="language-python">import pandas as pd

# Load dataset
h1b_data = pd.read_csv(&#39;h1b_2011_2016.csv&#39;)

# Display first few rows
print(h1b_data.head())
</code></pre>
<h2 is-upgraded>Step 3: Explore key columns</h2>
<p>Inspect important columns such as:</p>
<ul>
<li><code>CASE_STATUS</code> (visa application status)</li>
<li><code>EMPLOYER_NAME</code></li>
<li><code>SOC_NAME</code> (Standard Occupational Classification)</li>
<li><code>JOB_TITLE</code></li>
<li><code>FULL_TIME_POSITION</code></li>
<li><code>PREVAILING_WAGE</code></li>
<li><code>YEAR</code></li>
<li><code>WORKSITE</code> (city and state)</li>
<li><code>LON</code> and <code>LAT</code> (geographical coordinates)</li>
</ul>
<pre><code language="language-python" class="language-python">print(h1b_data.columns)
print(h1b_data[&#39;CASE_STATUS&#39;].value_counts())
print(h1b_data[[&#39;WORKSITE&#39;, &#39;LON&#39;, &#39;LAT&#39;]].head())
</code></pre>
<p><strong>Tip:</strong> Understanding the data distribution helps in planning preprocessing and feature engineering.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Data Cleaning and Preprocessing" duration="0">
        <h2 is-upgraded>Step 4: Clean the dataset</h2>
<ul>
<li>Drop unnecessary columns like <code>Unnamed: 0</code>.</li>
<li>Remove rows with missing values.</li>
<li>Split <code>WORKSITE</code> into <code>CITY</code> and <code>STATE</code>.</li>
<li>Binarize the target variable <code>CASE_STATUS</code> into <code>ACCEPT_REJECT</code>.</li>
</ul>
<pre><code language="language-python" class="language-python"># Drop unnamed column
h1b_data = h1b_data.drop(columns=[&#39;Unnamed: 0&#39;], errors=&#39;ignore&#39;)

# Drop rows with missing values
h1b_data = h1b_data.dropna()

# Split WORKSITE into CITY and STATE
h1b_data[[&#39;CITY&#39;, &#39;STATE&#39;]] = h1b_data[&#39;WORKSITE&#39;].str.split(&#39;,&#39;, expand=True)

# Binarize CASE_STATUS
h1b_data[&#39;ACCEPT_REJECT&#39;] = h1b_data[&#39;CASE_STATUS&#39;].apply(lambda x: 1 if x == &#39;CERTIFIED&#39; else 0)

print(h1b_data[[&#39;CASE_STATUS&#39;, &#39;ACCEPT_REJECT&#39;]].head())
</code></pre>
<h2 is-upgraded>Step 5: Clean text columns</h2>
<ul>
<li>Remove punctuation</li>
<li>Convert text to uppercase</li>
</ul>
<pre><code language="language-python" class="language-python">import string

def clean_text(text):
    if isinstance(text, str):
        text = text.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation))
        return text.upper()
    return text

text_columns = [&#39;SOC_NAME&#39;, &#39;JOB_TITLE&#39;, &#39;EMPLOYER_NAME&#39;, &#39;CITY&#39;]
for col in text_columns:
    h1b_data[col] = h1b_data[col].apply(clean_text)

print(h1b_data[text_columns].head())
</code></pre>
<h2 is-upgraded>Step 6: Handle high cardinality categorical features</h2>
<p>Group low-frequency categories into &#34;OTHERS&#34; to reduce noise.</p>
<pre><code language="language-python" class="language-python">for col in [&#39;SOC_NAME&#39;, &#39;JOB_TITLE&#39;, &#39;EMPLOYER_NAME&#39;, &#39;CITY&#39;]:
    freq = h1b_data[col].value_counts(normalize=True)
    mask = h1b_data[col].isin(freq[freq &lt; 0.01].index)
    h1b_data.loc[mask, col] = &#39;OTHERS&#39;

print(h1b_data[text_columns].nunique())
</code></pre>
<h2 is-upgraded>Step 7: Encode categorical variables</h2>
<p>Convert categorical columns to pandas category dtype and encode as integer codes.</p>
<pre><code language="language-python" class="language-python">for col in text_columns + [&#39;STATE&#39;]:
    h1b_data[col] = h1b_data[col].astype(&#39;category&#39;).cat.codes

print(h1b_data.dtypes)
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Feature Selection and Multicollinearity Analysis" duration="0">
        <h2 is-upgraded>Step 8: Analyze feature relationships</h2>
<p>Calculate correlation and covariance matrices.</p>
<pre><code language="language-python" class="language-python">corr_matrix = h1b_data.corr()
print(corr_matrix[&#39;ACCEPT_REJECT&#39;].sort_values(ascending=False))
</code></pre>
<h2 is-upgraded>Step 9: Calculate Variance Inflation Factor (VIF)</h2>
<p>Detect multicollinearity and drop features with high VIF.</p>
<pre><code language="language-python" class="language-python">from statsmodels.stats.outliers_influence import variance_inflation_factor
import numpy as np

features = h1b_data.drop(columns=[&#39;CASE_STATUS&#39;, &#39;ACCEPT_REJECT&#39;, &#39;WORKSITE&#39;])

vif_data = pd.DataFrame()
vif_data[&#39;feature&#39;] = features.columns
vif_data[&#39;VIF&#39;] = [variance_inflation_factor(features.values, i) for i in range(len(features.columns))]

print(vif_data)

# Drop features with high VIF
features = features.drop(columns=[&#39;YEAR&#39;, &#39;FULL_TIME_POSITION&#39;, &#39;LON&#39;, &#39;LAT&#39;])
</code></pre>
<h2 is-upgraded>Step 10: Select top features using SelectKBest</h2>
<pre><code language="language-python" class="language-python">from sklearn.feature_selection import SelectKBest, chi2

X = features
y = h1b_data[&#39;ACCEPT_REJECT&#39;]

selector = SelectKBest(chi2, k=6)
X_new = selector.fit_transform(X, y)

selected_features = X.columns[selector.get_support()]
print(&#39;Selected features:&#39;, list(selected_features))
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Data Splitting and Balancing" duration="0">
        <h2 is-upgraded>Step 11: Split data into training and testing sets</h2>
<pre><code language="language-python" class="language-python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.2, random_state=42)

print(&#39;Training set size:&#39;, X_train.shape)
print(&#39;Test set size:&#39;, X_test.shape)
</code></pre>
<h2 is-upgraded>Step 12: Address class imbalance with SMOTE</h2>
<pre><code language="language-python" class="language-python">from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

print(&#39;Balanced training set class distribution:&#39;)
print(pd.Series(y_train_bal).value_counts())
</code></pre>
<p><strong>Warning:</strong> Balancing the dataset helps models learn minority classes better but be cautious of overfitting.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Model Building" duration="0">
        <h2 is-upgraded>Step 13: Train multiple classification models</h2>
<p>Train KNN, Decision Tree, Random Forest, and XGBoost models.</p>
<pre><code language="language-python" class="language-python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

# Initialize models
knn = KNeighborsClassifier()
dt = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(random_state=42)

# Train models
knn.fit(X_train_bal, y_train_bal)
dt.fit(X_train_bal, y_train_bal)
rf.fit(X_train_bal, y_train_bal)

# XGBoost with custom modelfit function

def modelfit(alg, X_train, y_train, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(X_train, label=y_train)
        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()[&#39;n_estimators&#39;],
                          nfold=cv_folds, metrics=&#39;auc&#39;, early_stopping_rounds=early_stopping_rounds, seed=42)
        alg.set_params(n_estimators=cvresult.shape[0])
    alg.fit(X_train, y_train, eval_metric=&#39;auc&#39;)

xgb_model = xgb.XGBClassifier(n_estimators=1000, max_depth=5, learning_rate=0.1, random_state=42)
modelfit(xgb_model, X_train_bal, y_train_bal)
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Model Evaluation" duration="0">
        <h2 is-upgraded>Step 14: Evaluate models on test set</h2>
<p>Calculate confusion matrix, accuracy, precision, recall, F1 score, and classification report.</p>
<pre><code language="language-python" class="language-python">from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

models = {&#39;KNN&#39;: knn, &#39;Decision Tree&#39;: dt, &#39;Random Forest&#39;: rf, &#39;XGBoost&#39;: xgb_model}

for name, model in models.items():
    y_pred = model.predict(X_test[selected_features])
    print(f&#39;--- {name} Model Evaluation ---&#39;)
    print(&#39;Confusion Matrix:&#39;)
    print(confusion_matrix(y_test, y_pred))
    print(&#39;Accuracy:&#39;, accuracy_score(y_test, y_pred))
    print(&#39;Precision:&#39;, precision_score(y_test, y_pred))
    print(&#39;Recall:&#39;, recall_score(y_test, y_pred))
    print(&#39;F1 Score:&#39;, f1_score(y_test, y_pred))
    print(&#39;Classification Report:&#39;)
    print(classification_report(y_test, y_pred))

    # ROC Curve
    y_proba = model.predict_proba(X_test[selected_features])[:,1]
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    auc_score = roc_auc_score(y_test, y_proba)
    plt.plot(fpr, tpr, label=f&#39;{name} (AUC = {auc_score:.2f})&#39;)

plt.plot([0,1], [0,1], &#39;k--&#39;)
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.title(&#39;ROC Curves&#39;)
plt.legend()
plt.show()
</code></pre>
<p><strong>Insight:</strong> XGBoost typically achieves the best performance with ~91.6% accuracy and AUC ~0.83.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Prediction Examples" duration="0">
        <h2 is-upgraded>Step 15: Predict visa approval probabilities</h2>
<p>Use trained models to predict acceptance probabilities for sample inputs.</p>
<pre><code language="language-python" class="language-python">sample_input = X_test[selected_features].iloc[0:3]

print(&#39;Random Forest Predictions:&#39;)
print(rf.predict_proba(sample_input))

print(&#39;XGBoost Predictions:&#39;)
print(xgb_model.predict_proba(sample_input))
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Model Persistence" duration="0">
        <h2 is-upgraded>Step 16: Save the final model</h2>
<p>Save the Random Forest model for future use.</p>
<pre><code language="language-python" class="language-python">import pickle

with open(&#39;h1b_prediction_model_rf2.pk&#39;, &#39;wb&#39;) as f:
    pickle.dump(rf, f)

print(&#39;Model saved as h1b_prediction_model_rf2.pk&#39;)
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Summary" duration="0">
        <p>Congratulations! You have completed the H-1B Visa Approval Prediction project codelab. You have learned how to:</p>
<ul>
<li>Load and preprocess a complex dataset</li>
<li>Handle high cardinality categorical features</li>
<li>Address class imbalance with SMOTE</li>
<li>Train and evaluate multiple classification models</li>
<li>Save a trained model for deployment</li>
</ul>
<p>Feel free to explore the backend and frontend components (<code>backend/app.py</code> and <code>index.html</code>) to build a web application interface for your model.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Contact and Contribution" duration="0">
        <p>For questions, suggestions, or contributions, please open an issue or submit a pull request in the repository.</p>
<p>Thank you for exploring the H-1B Visa Approval Prediction project!</p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
